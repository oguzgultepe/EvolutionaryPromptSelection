{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Evolutionary Prompt Selection\n## For Planner/Worker/Solver Framework","metadata":{}},{"cell_type":"markdown","source":"### Install Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install accelerate\n!pip install bitsandbytes\n!pip install sentencepiece\n!pip install torch\n!pip install transformers\n!pip install wikipedia\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Prompt Prefixes/Suffixes","metadata":{}},{"cell_type":"code","source":"PLANNER_PROMPT = {\"prefix\": \"For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...) \\n\\n\",\"suffix\": \"Begin! Describe your plans with rich details. Each Plan should be followed by only one #E.\\n\\n\"}\nSOLVER_PROMPT = {\"prefix\": \"Solve the following task or problem. To assist you, we provide some plans and corresponding evidences that might be helpful. Notice that some of these information contain noise so you should trust them with caution.\\n\\n\", \"suffix\": \"\\nNow begin to solve the task or problem. Respond with the answer directly with no extra words.\\n\\n\"}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define classes and functions","metadata":{}},{"cell_type":"markdown","source":"#### Nodes","metadata":{}},{"cell_type":"code","source":"import wikipedia\n\nclass Node:\n    \"\"\"Basic node class\"\"\"\n    def __init__(self):\n        raise NotImplementedError\n\n    def run (self, inputs):\n        raise NotImplementedError\n\nclass LLMNode(Node):\n    \"\"\"A node that is based on an LLM\"\"\"\n    def __init__(self, model):\n        self.model = model\n\n    def call_llm(self, prompt):\n        \"\"\"Calls the underlying LLM with the given inputs\n        Parameters:\n        ------------\n        prompt: str\n            prompt for the LLM\n\n        Returns:\n        ------------\n        response: str\n            LLM response\n        \"\"\"\n        response = self.model.generate(prompt)\n        return response\n\nclass Planner(LLMNode):\n    \"\"\"Planner node for making plans within the PWS framework\"\"\"\n    def __init__(self, model):\n        self.prefix = PLANNER_PROMPT['prefix']\n        self.suffix = PLANNER_PROMPT['suffix']\n        self.model = model\n\n    def run(self, task, examples, tools):\n        \"\"\"Generate plans for the given task, examples and tools\n        Parameters:\n        ------------\n        task: str\n            Task for which the plan is to be generated\n        examples: list(str)\n            Examples related to the task for the fewshot prompt\n        tools: dict(str:str)\n            Tools that can be used to solve the task\n\n        Returns:\n        ------------\n        planner_response: dict(str:obj)\n            Planner response contains the plans and the evidences\n        \"\"\"\n        prompt = self.generate_prompt(task, examples, tools)\n        response = self.call_llm(prompt)\n        plans, tool_calls = self.parse_response(response)\n        planner_response = {'plans': plans, 'tool_calls': tool_calls}\n        return planner_response\n\n    def generate_prompt(self, task, examples, tools):\n        \"\"\"Generates a planner prompt for the given task, examples and tools\n        Parameters:\n        ------------\n        task: str\n            Task for which the plan is to be generated\n        examples: list(str)\n            Examples related to the task for the fewshot prompt\n        tools: dict(str:str)\n            Tools that can be used to solve the task\n\n        Returns:\n        ------------\n        prompt: str\n            planner prompt\n        \"\"\"\n        prompt = self.prefix\n        prompt += self.generate_worker_prompt(tools)\n        prompt += '\\n\\n'.join(examples)\n        prompt += self.suffix\n        prompt += f\"Question: {task}\\n\"\n        return prompt\n\n    def generate_worker_prompt(self, tools):\n        \"\"\"Generates a worker prompt for given tools\n        Parameters:\n        ------------\n        tools: dict(str:str)\n            contains the names and descriptions of the tools\n\n        Returns:\n        ------------\n        prompt: str\n            worker prompt\n        \"\"\"\n        prompt = \"Tools can be one of the following:\\n\"\n        for tool, description in tools.items():\n            prompt += f\"{tool}[input]: {description}\\n\"\n        return prompt + \"\\n\"\n\n    def parse_response(self, response):\n        \"\"\"Parse the planner response and return plans and evidences dictionary\n        Parameters:\n        ------------\n        response: str\n            Planner response\n\n        Returns:\n        ------------\n        plans: list(str)\n            List that contains the plans\n        evidences: dict(str:str)\n            Evidence dict conatining evidences and associated tool calls\n        \"\"\"\n        plans = []\n        tool_calls = {}\n        for line in response.splitlines():\n            if line.startswith(\"Plan:\"):\n                plans.append(line)\n            elif line.startswith(\"#\") and line[1] == \"E\" and line[2].isdigit():\n                e, tool_call = line.split(\"=\", 1)\n                e, tool_call = e.strip(), tool_call.strip()\n                if len(e) == 3:\n                    tool_calls[e] = tool_call\n                else:\n                    tool_calls[e] = \"No evidence found\"\n        return plans, tool_calls\n\nclass WikipediaWorker(Node):\n    \"\"\"Worker that searches Wikipedia\"\"\"\n    def __init__(self):\n        pass\n\n    def run(self, inputs):\n        \"\"\"Searches Wikipedia for the given inputs and returns the first\n        paragraph of the first page in search results\n        Parameters:\n        ------------\n        inputs: str\n            String input for Wikipedia search\n\n        Returns:\n        ------------\n        evidence: str\n            First paragraph of the first page from the search results\n        \"\"\"\n        pages = wikipedia.search(inputs, results=1)\n        if pages:\n            content = wikipedia.page(pages[0], auto_suggest=False).content\n            evidence = content.split('\\n\\n', 1)[0]\n        else:\n            evidence = \"No evidence found.\"\n        return evidence\n\nclass LLMWorker(LLMNode):\n    \"\"\"LLM node to be used for worker calls\"\"\"\n    def run(self, inputs):\n        \"\"\"Run the LLM as a tool call\n        Parameters:\n        ------------\n        inputs: str\n            Input for the tool call\n\n        Returns:\n        ------------\n        evidence: str\n            Cleaned response from the tool call\n        \"\"\"\n        prompt = f\"Respond in short directly with no extra words.\\n\\n{inputs}\\n\\n\"\n        response = self.call_llm(prompt)\n        evidence = response.strip(\"\\n\")\n        return evidence\n\nclass Worker(Node):\n    \"\"\"Worker node that calls appropriate workers for each tool call\"\"\"\n    def __init__(self, model):\n        self.wiki_worker = WikipediaWorker()\n        self.llm_worker = LLMWorker(model)\n\n    def run(self, inputs):\n        \"\"\"Faciliates all tool calls and returns evidences\n        Parameters:\n        ------------\n        inputs: dict(str:str)\n            A dictionary of evidence variables and associated tool calls\n\n        Returns:\n        ------------\n        evidences: dict(str:str)\n            A dictinary of evidence variables and the outputs of the associated\n            tool calls\n        \"\"\"\n        evidences = {}\n        for e, tool_call in inputs.items():\n            # Do not process tools without input\n            if \"[\" not in tool_call:\n                evidences[e] = tool_call\n                continue\n\n            # Seperate tool and tool input\n            tool, tool_input = tool_call.split(\"[\", 1)\n            tool_input = tool_input[:-1]\n\n            # Find variables in input and replace with previous evidences\n            for var in re.findall(r\"#E\\d+\", tool_input):\n                if var in evidences:\n                    try:\n                        evidence = evidences[var]\n                    except KeyError:\n                        evidence = \"No evidence found.\"\n                    tool_input = tool_input.replace(var, f\"[{evidence}]\")\n\n            match tool:\n                case \"Wikipedia\":\n                    evidences[e] = self.wiki_worker.run(tool_input)\n                case \"LLM\":\n                    evidences[e] = self.llm_worker.run(tool_input)\n                case _:\n                    evidences[e] = \"No evidence found.\"\n\n        return evidences\n\nclass Solver(LLMNode):\n    \"\"\"Solver node that solves tasks for given plans and evidences\"\"\"\n    def __init__(self, model):\n        self.prefix = SOLVER_PROMPT['prefix']\n        self.suffix = SOLVER_PROMPT['suffix']\n        self.model = model\n\n    def run(self, task, plans, evidences):\n        \"\"\"Solve the task based on the given plans and evidences\n        Parameters:\n        ------------\n        task: str\n            Task to be solved\n        plans: list(str)\n            List of plans generated by Planner\n        evidences: dict(str:str)\n            Dictionary of evidences generated by the Worker\n\n        Returns:\n        ------------\n        output: str\n            Solution generated based on the given plans and evidences\n        \"\"\"\n        prompt = self.prefix\n        prompt += task + '\\n'\n        for i in range(len(plans)):\n            e = f\"#E{i + 1}\"\n            plan = plans[i]\n            try:\n              evidence = evidences[e]\n            except KeyError:\n              evidence = \"No evidence found.\"\n            prompt += f\"{plan}\\nEvidence:\\n{evidence}\\n\"\n        prompt += self.suffix\n        prompt += task + '\\n\\n'\n        output = self.call_llm(prompt)\n        return output\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Utils","metadata":{}},{"cell_type":"code","source":"import re\nimport time\nimport torch\nfrom transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\nclass ParagraphStoppingCriteria(StoppingCriteria):\n    \"\"\"Stops generating after double newline.\"\"\"\n    def __init__(self, newline_id):\n        self.newline = newline_id\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        if input_ids[0][-1] == self.newline \\\n        and input_ids[0][-2] == self.newline:\n\n            return True\n        return False\n        \nclass LanguageModel:\n    \"\"\"Language model wrapper to be used in nodes\"\"\"\n    def __init__(self, model_path, generation_config):\n        self.tokenizer = LlamaTokenizer.from_pretrained(model_path)\n        self.model = LlamaForCausalLM.from_pretrained(\n            model_path, torch_dtype=torch.float16, device_map='auto')\n        self.generation_config = generation_config\n        newline_id = self.tokenizer.encode('\\n')[-1]\n        self.stopping_criteria = StoppingCriteriaList([ParagraphStoppingCriteria(newline_id)])\n    \n    def generate(self, prompt):\n        \"\"\"Generate text based on given prompt\n        Parameters:\n        ------------\n        prompt: str\n            Prompt for thee LLM\n\n        Returns:\n        ------------\n        llm_response: str\n            LLM generated response\n        \"\"\"\n        tokens = self.tokenizer.encode(prompt)\n        tokens = torch.LongTensor(tokens).unsqueeze(0)\n        tokens = tokens.to('cuda')\n\n        length = len(tokens[0])\n        with torch.no_grad():\n            rest = self.model.generate(\n                input_ids=tokens,\n                generation_config=self.generation_config,\n                stopping_criteria=self.stopping_criteria\n            )\n        output = rest[0][length:]\n        llm_response = self.tokenizer.decode(output, skip_special_tokens=True)\n        return llm_response\n\nclass PWS:\n    \"\"\" Planner Worker Solver Framework\"\"\"\n    def __init__(self, model):\n        self.planner = Planner(model=model)\n        self.worker = Worker(model=model)\n        self.solver = Solver(model=model)\n\n    def run(self, task, examples, tools, verbose=False):\n        \"\"\"Run the PWS on a given task based on provided examples/tools\n        Parameters:\n        ------------\n        task: str\n            Task for which the PWS is to be run\n        examples: list(str)\n            Examples related to the task for the fewshot prompt\n        tools: dict(str:str)\n            Tools that can be used to solve the task\n\n        Returns:\n        ------------\n        pws_response: dict(str:obj)\n            PWS response contains the output and time elapsed\n            If verbose responses from intermediate nodes are also returned\n        \"\"\"\n\n        st = time.time()\n        # Plan\n        planner_response = self.planner.run(task, examples, tools)\n        plans = planner_response[\"plans\"]\n        tool_calls = planner_response[\"tool_calls\"]\n\n        # Work\n        evidences = self.worker.run(tool_calls)\n\n        # Solve\n        output = self.solver.run(task, plans, evidences)\n\n        wall_time = time.time() - st\n\n        pws_response = {\"output\": output,\n                        \"wall_time\": wall_time}\n\n        if verbose:\n            pws_response[\"planner_response\"] = planner_response\n            pws_response[\"worker_response\"] = evidences\n\n        return pws_response\n\n\nclass EPS:\n    \"\"\" Evolutionary Prompt Selection\"\"\"\n    #TODO\n    def __init__(self):\n        raise NotImplementedError\n    def select_examples(self, task, num_examples):\n        raise NotImplementedError\n        # return examples, tools\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Initialize the model","metadata":{}},{"cell_type":"code","source":"from transformers import GenerationConfig\n\nMODEL_PATH = \"lmsys/vicuna-7b-v1.3\"\nTEMPERATURE = 0.7\nTOP_K = 100\nTOP_P = 0.7\nREPETITION_PENALTY= 1.0\nMAX_NEW_TOKENS = 512\n\ngeneration_config = GenerationConfig(\n    do_sample=True,\n    temperature=TEMPERATURE,\n    top_k=TOP_K,\n    top_p=TOP_P,\n    repetition_penalty=REPETITION_PENALTY,\n    max_new_tokens=MAX_NEW_TOKENS\n)\n\nmodel = LanguageModel(MODEL_PATH, generation_config=generation_config)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Initialize Evolutionary Prompt Selector","metadata":{}},{"cell_type":"code","source":"# TODO\n# EPS()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Example/tool dummies\nRemove after EPS is implemented","metadata":{}},{"cell_type":"code","source":"tools = {\"Wikipedia\": \"Worker that search for similar page contents from Wikipedia. Useful when you need to get holistic knowledge about people, places, companies, historical events, or other subjects. The response are long and might contain some irrelevant information. Input should be a search query.\",\n\"LLM\": \"A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\"}\n\nexamples = [\"\"\"Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\nPlan: Search for more information about Colorado orogeny.\n#E1 = Wikipedia[Colorado orogeny]\nPlan: Find out the area that eastern sector of the Colorado orogeny extends into.\n#E2 = LLM[What is the name of the area that eastern sector of Colorado extends into? Given context: #E1]\nPlan: Search for more information about the area.\n#E3 = Wikipedia[#E2]\nPlan: Find out the elevation range for the area.\n#E4 = LLM[What is elevation range for the area #E2? Given context: #E3]\"\"\",\n\"\"\"Question: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\nPlan: Search for more information about Milhouse.\n#E1 = Wikipedia[Milhouse]\nPlan: Find out who Matt Groening named Milhouse after.\n#E2 = LLM[Who did Matt Groening name Milhouse after? Given context: #E1]\"\"\",\n\"\"\"Question: Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?\nPlan: Search for more information about Adam Clayton Powell.\n#E1 = Wikipedia[Adam Clayton Powell]\nPlan: Search for more information about The Saimaa Gesture.\n#E2 = Wikipedia[The Saimaa Gesture]\nPlan: Compare the two and determine which is a documentary about Finnish rock groups.\n#E3 = LLM[Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture? Given context: #E1, #E2]\"\"\",\n\"\"\"Question: What profession does Nicholas Ray and Elia Kazan have in common?\nPlan: Search for more information about Nicholas Ray.\n#E1 = Wikipedia[Nicholas Ray]\nPlan: Search for more information about Elia Kazan.\n#E2 = Wikipedia[Elia Kazan]\nPlan: Compare the two and determine what profession they have in common.\n#E3 = LLM[What profession does Nicholas Ray and Elia Kazan have in common? Given context: #E1, #E2]\"\"\",\n\"\"\"Question: Which magazine was started first Arthur's Magazine or First for Women?\nPlan: Search for more information about Arthur's Magazine.\n#E1 = Wikipedia[Arthur's Magazine]\nPlan: Search for more information about First for Women.\n#E2 = Wikipedia[First for Women]\nPlan: Compare the two start dates and determine which magazine was started first.\n#E3 = LLM[Which magazine was started first Arthur's Magazine or First for Women? Given context: #E1, #E2]\"\"\",\n\"\"\"Question: Were Pavel Urysohn and Leonid Levin known for the same type of work?\nPlan: Search for more information about Pavel Urysohn.\n#E1 = Wikipedia[Pavel Urysohn]\nPlan: Search for more information about Leonid Levin.\n#E2 = Wikipedia[Leonid Levin]\nPlan: Compare the two and determine if they were known for the same type of work.\n#E3 = LLM[Were Pavel Urysohn and Leonid Levin known for the same type of work? Given context: #E1, #E2]\"\"\"]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Answer questions","metadata":{"execution":{"iopub.status.busy":"2023-07-17T21:57:40.661014Z","iopub.execute_input":"2023-07-17T21:57:40.661426Z","iopub.status.idle":"2023-07-17T21:57:40.669885Z","shell.execute_reply.started":"2023-07-17T21:57:40.661391Z","shell.execute_reply":"2023-07-17T21:57:40.668734Z"}}},{"cell_type":"code","source":"agent = PWS(model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#task = input(\"Enter your question...\")\ntask = \"Which city is situated near Thames?\"\n#examples, tools = EPS.select_examples(task, num_examples=3)\nresponse = agent.run(task, examples, tools, verbose=True)\nprint('response'['output'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment below this block","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}