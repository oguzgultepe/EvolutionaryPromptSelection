{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Evolutionary Prompt Selection\n## For Planner/Worker/Solver Framework","metadata":{}},{"cell_type":"markdown","source":"### Install Dependencies","metadata":{}},{"cell_type":"code","source":"!printf 'accelerate\\nbitsandbytes\\ndatasets\\npinecone-client[grpc]\\nsentencepiece\\nsentence-transformers\\ntorch\\ntransformers\\nwikipedia ' > requirements.txt  \n!pip install -r requirements.txt\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Statements","metadata":{"execution":{"iopub.status.busy":"2023-08-01T14:30:56.361309Z","iopub.execute_input":"2023-08-01T14:30:56.361651Z","iopub.status.idle":"2023-08-01T14:30:56.369346Z","shell.execute_reply.started":"2023-08-01T14:30:56.361606Z","shell.execute_reply":"2023-08-01T14:30:56.368460Z"}}},{"cell_type":"code","source":"import json\nimport math\nimport os\nimport re\nimport string\nimport time\n\nimport pandas as pd\nimport pinecone\nimport torch\nimport wikipedia\n\nfrom threading import Thread, Lock\n\nfrom datasets import load_dataset\nfrom numpy.random import choice\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\nfrom transformers import StoppingCriteria, StoppingCriteriaList\nfrom tqdm.auto import tqdm\n\nfrom kaggle_secrets import UserSecretsClient\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Prompt Prefixes/Suffixes","metadata":{}},{"cell_type":"code","source":"PLANNER_PROMPT = {\"prefix\": \"You are an advanced AI capable of making plans to solve complex problems. For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\",\"suffix\": \"Describe your plans with rich details. Each Plan should be followed by only one #E. Answer each question directly with plans.\"}\nSOLVER_PROMPT = {\"prefix\": \"You are an advanced AI capable of solving tasks based on evidence. Solve the following task or problem based on the provided plans and corresponding evidences. Keep your responses direct and concise.\", \"suffix\": \"Based on the provided evidence answer the following question directly and concisely: \"}\nTOOLS_PROMPT = {\"Wikipedia\": \"Worker that search for similar page contents from Wikipedia. Useful when you need to get holistic knowledge about people, places, companies, historical events, or other subjects. The response are long and might contain some irrelevant information. Input should be a search query.\", \"LLM\": \"A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\"}\nEXTRACTOR_PROMPT = {\"prefix\": \"Based on the given statement, concisely extract the answer to the following question. Respond directly with the concise answer to the question.\"}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define classes and functions","metadata":{}},{"cell_type":"markdown","source":"#### Nodes","metadata":{}},{"cell_type":"code","source":"class Node:\n    \"\"\"Basic node class\"\"\"\n    def __init__(self):\n        raise NotImplementedError\n\n    def run (self, inputs):\n        raise NotImplementedError\n\n\nclass LLMNode(Node):\n    \"\"\"A node that is based on an LLM\"\"\"\n    def __init__(self, model):\n        self.model = model\n        self.system_tag = model.system_tag\n        self.user_tag = model.user_tag\n        self.ai_tag = model.ai_tag\n        self.stops = ['.', '\\n']\n\n    def call_llm(self, prompt):\n        \"\"\"Calls the underlying LLM with the given inputs\n        Parameters:\n        ------------\n        prompt: str\n            prompt for the LLM\n\n        Returns:\n        ------------\n        response: str\n            LLM response\n        \"\"\"\n        response = self.model.generate(prompt, self.stops)\n        return response\n\n\nclass Planner(LLMNode):\n    \"\"\"Planner node for making plans within the PWS framework\"\"\"\n    def __init__(self, model):\n        super().__init__(model)\n        self.stops = ['\\n\\n']\n        self.prefix = PLANNER_PROMPT['prefix']\n        self.suffix = PLANNER_PROMPT['suffix']\n        self.tools = TOOLS_PROMPT\n\n    def run(self, task, examples):\n        \"\"\"Generate plans for the given task, examples and tools\n        Parameters:\n        ------------\n        task: str\n            Task for which the plan is to be generated\n        examples: list(dict)\n            Examples related to the task for the fewshot prompt\n\n        Returns:\n        ------------\n        planner_response: dict(str:obj)\n            Planner response contains the plans and the evidences\n        \"\"\"\n        prompt = self.generate_prompt(task, examples)\n        response = self.call_llm(prompt)\n        plans, tool_calls = self.parse_response(response)\n        planner_response = {'plans': plans, 'tool_calls': tool_calls,\n                            'text':response}\n        return planner_response\n\n    def generate_prompt(self, task, examples):\n        \"\"\"Generates a planner prompt for the given task, examples and tools\n        Parameters:\n        ------------\n        task: str\n            Task for which the plan is to be generated\n        examples: list(dict)\n            Examples related to the task for the fewshot prompt\n\n        Returns:\n        ------------\n        prompt: str\n            planner prompt\n        \"\"\"\n        tools = {tool: self.tools[tool] for example in examples\n                 for tool in example['tools']}\n\n        prompt = f\"{self.system_tag}{self.prefix}\\n\"\n        prompt += \"Tools can be one of the following:\\n\"\n        for tool, description in tools.items():\n            prompt += f\"{tool}[input]: {description}\\n\"\n        prompt += f\"{self.suffix}\\n\\n\"\n        for example in examples:\n            prompt += f\"{self.user_tag}{example['question'].strip()}\\n\\n\"\n            prompt += f\"{self.ai_tag}{example['plan'].strip()}\\n\\n\"\n        prompt += f\"{self.user_tag}{task.strip()}\\n\\n\"\n        prompt += self.ai_tag\n        return prompt\n\n    def parse_response(self, response):\n        \"\"\"Parse the planner response and return plans and evidences dictionary\n        Parameters:\n        ------------\n        response: str\n            Planner response\n\n        Returns:\n        ------------\n        plans: list(str)\n            List that contains the plans\n        evidences: dict(str:str)\n            Evidence dict conatining evidences and associated tool calls\n        \"\"\"\n        plans = []\n        tool_calls = {}\n        for line in response.splitlines():\n            if line.startswith(\"Plan:\"):\n                plans.append(line)\n            elif len(line) < 3:\n                continue\n            elif line.startswith(\"#\") and line[1] == \"E\" and line[2].isdigit():\n                e, tool_call = line.split(\"=\", 1)\n                e, tool_call = e.strip(), tool_call.strip()\n                if len(e) == 3:\n                    tool_calls[e] = tool_call\n                else:\n                    tool_calls[e] = \"No evidence found\"\n        return plans, tool_calls\n\n\nclass WikipediaWorker(Node):\n    \"\"\"Worker that searches Wikipedia\"\"\"\n    def __init__(self):\n        pass\n\n    def run(self, inputs):\n        \"\"\"Searches Wikipedia for the given inputs and returns the first\n        2000 characters of the first page in the search results\n        Parameters:\n        ------------\n        inputs: str\n            String input for Wikipedia search\n\n        Returns:\n        ------------\n        evidence: str\n            First paragraph of the first page from the search results\n        \"\"\"\n        evidence = \"No evidence found.\"\n        pages = wikipedia.search(inputs[:300], results=1)\n        if pages:\n            try:\n                evidence = wikipedia.page(pages[0], auto_suggest=False).content\n                evidence = evidence[:2000]\n            except:\n                pass\n\n        return evidence\n\n\nclass LLMWorker(LLMNode):\n    \"\"\"LLM node to be used for worker calls\"\"\"\n    def run(self, inputs):\n        \"\"\"Run the LLM as a tool call\n        Parameters:\n        ------------\n        inputs: str\n            Input for the tool call\n\n        Returns:\n        ------------\n        evidence: str\n            Cleaned response from the tool call\n        \"\"\"\n        # Truncate input if necessary\n        tokens = self.model.tokenizer(inputs)['input_ids']\n        if len(tokens) > 2000:\n            inputs = self.model.tokenizer.decode(tokens[:2000],\n                                                 skip_special_tokens=True)\n        prompt = self.system_tag\n        prompt += \"Directly answer the following question with no extra words.\"\n        prompt += f\"\\n\\n{self.user_tag}{inputs.strip()}\\n\\n{self.ai_tag}\"\n        response = self.call_llm(prompt)\n        evidence = response.strip()\n        return evidence\n\n\nclass Worker(Node):\n    \"\"\"Worker node that calls appropriate workers for each tool call\"\"\"\n    def __init__(self, model):\n        self.wiki_worker = WikipediaWorker()\n        self.llm_worker = LLMWorker(model)\n\n    def run(self, inputs):\n        \"\"\"Faciliates all tool calls and returns evidences\n        Parameters:\n        ------------\n        inputs: dict(str:str)\n            A dictionary of evidence variables and associated tool calls\n\n        Returns:\n        ------------\n        evidences: dict(str:str)\n            A dictinary of evidence variables and the outputs of the associated\n            tool calls\n        \"\"\"\n        evidences = {}\n        for e, tool_call in inputs.items():\n            # Do not process tools without input\n            if \"[\" not in tool_call:\n                evidences[e] = tool_call\n                continue\n\n            # Seperate tool and tool input\n            tool, tool_input = tool_call.split(\"[\", 1)\n            tool_input = tool_input[:-1]\n\n            # Find variables in input and replace with previous evidences\n            for var in re.findall(r\"#E\\d+\", tool_input):\n                if var in evidences:\n                    try:\n                        evidence = evidences[var]\n                    except KeyError:\n                        evidence = \"No evidence found.\"\n                    tool_input = tool_input.replace(var, f\"[{evidence}]\")\n\n            match tool:\n                case \"Wikipedia\":\n                    evidences[e] = self.wiki_worker.run(tool_input)\n                case \"LLM\":\n                    evidences[e] = self.llm_worker.run(tool_input)\n                case _:\n                    evidences[e] = \"No evidence found.\"\n\n        return evidences\n\n\nclass Solver(LLMNode):\n    \"\"\"Solver node that solves tasks for given plans and evidences\"\"\"\n    def __init__(self, model):\n        super().__init__(model)\n        self.prefix = SOLVER_PROMPT['prefix']\n        self.suffix = SOLVER_PROMPT['suffix']\n\n    def run(self, task, plans, evidences):\n        \"\"\"Solve the task based on the given plans and evidences\n        Parameters:\n        ------------\n        task: str\n            Task to be solved\n        plans: list(str)\n            List of plans generated by Planner\n        evidences: dict(str:str)\n            Dictionary of evidences generated by the Worker\n\n        Returns:\n        ------------\n        output: str\n            Solution generated based on the given plans and evidences\n        \"\"\"\n        prompt = f\"{self.system_tag}{self.prefix}\\n\\n\"\n        prompt += f\"{self.user_tag}{task.strip()}\\n\"\n        for i in range(len(plans)):\n            e = f\"#E{i + 1}\"\n            plan = plans[i]\n            try:\n                evidence = evidences[e]\n            except KeyError:\n                evidence = \"No evidence found.\"\n            # Only include the first 500 characters of each evidence\n            prompt += f\"{plan}\\nEvidence: {evidence[:500]}...\\n\"\n        prompt += f\"{self.suffix + task.strip()}\\n\\n{self.ai_tag}\"\n        output = self.call_llm(prompt)\n        return output\n\nclass Extractor(LLMNode):\n    def __init__(self, model):\n        super().__init__(model)\n        self.prefix = EXTRACTOR_PROMPT['prefix']\n\n    def __call__(self, statement, question):\n        prompt = f\"{self.system_tag}{self.prefix}\\n\"\n        prompt += f\"{self.user_tag}Statement: {statement}\\n\"\n        prompt += f\"Question: {question}\\n{self.ai_tag}\"\n        output = self.call_llm(prompt)\n        return output\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Utils","metadata":{}},{"cell_type":"code","source":"class MultiTokenEOSCriteria(StoppingCriteria):\n    \"\"\"Stopping criteria based on a given multi-token sequence.\n    Please refer to HuggingFace Transformers library for documentation\"\"\"\n\n    def __init__(self, sequence, tokenizer, initial_decoder_input_length):\n        self.initial_decoder_input_length = initial_decoder_input_length\n        self.sequence = sequence\n        self.sequence_ids = tokenizer.encode(sequence,\n                                             add_special_tokens=False)\n        self.sequence_id_len = len(self.sequence_ids)\n        self.tokenizer = tokenizer\n\n    def __call__(self, input_ids, scores, **kwargs) -> bool:\n        # For efficiency, we compare the last n tokens where n is the number\n        # of tokens in the stop_sequence\n        lookback_ids = input_ids[0][self.initial_decoder_input_length:]\n        lookback_ids = lookback_ids[-self.sequence_id_len:]\n        lookback_tokens = self.tokenizer.decode(lookback_ids)\n        return self.sequence in lookback_tokens\n\n\nclass LanguageModel:\n    \"\"\"Language model wrapper to be used in nodes\"\"\"\n    def __init__(self, model_path, generation_config,\n                 device_map='auto', load_in_8bit=False, access_token=None,\n                 system_tag='\\n', user_tag='\\n', ai_tag='\\n'):\n        self.tokenizer = LlamaTokenizer.from_pretrained(model_path,\n            use_auth_token=access_token)\n        self.model = LlamaForCausalLM.from_pretrained(\n            model_path, torch_dtype=torch.float16, device_map=device_map,\n            load_in_8bit=load_in_8bit, use_auth_token=access_token)\n        self.generation_config = generation_config\n        if device_map == 'auto':\n            self.device = 'cuda'\n        else:\n            self.device = f'cuda:{device_map}'\n        self.system_tag = system_tag\n        self.user_tag = user_tag\n        self.ai_tag = ai_tag\n\n    def stop_sequences_criteria(self, stop_sequences,\n                                initial_decoder_input_length):\n        \"\"\"Creates a custom stopping criteria for the given input\n        Parameters:\n        ------------\n        stop_sequences: list(str)\n            A list of strings that ends text generation\n        initial_decoder_input_length: int\n            Total number of tokens in the initial input\n\n        Returns:\n        ------------\n            StoppingCriteriaList object\n        \"\"\"\n        return StoppingCriteriaList(\n            [\n                MultiTokenEOSCriteria(sequence, self.tokenizer,\n                                      initial_decoder_input_length)\n                for sequence in stop_sequences\n            ]\n        )\n\n    def generate(self, prompt, stops):\n        \"\"\"Generate text based on given prompt\n        Parameters:\n        ------------\n        prompt: str\n            Prompt for the LLM\n        stops: list(str)\n            List of strings to be used as stopping criteria\n\n        Returns:\n        ------------\n        output_text: str\n            LLM generated response\n        \"\"\"\n        input_tokens = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n        input_length = input_tokens['input_ids'].shape[1]\n        stopping_criteria = self.stop_sequences_criteria(stops, input_length)\n        with torch.no_grad():\n            output_tokens = self.model.generate(\n                **input_tokens,\n                generation_config=self.generation_config,\n                stopping_criteria=stopping_criteria\n                )\n\n        output_text = self.tokenizer.decode(output_tokens[0][input_length:],\n                                            skip_special_tokens=True)\n\n        return output_text\n\n\nclass PWS:\n    \"\"\" Planner Worker Solver Framework\"\"\"\n    def __init__(self, model):\n        self.planner = Planner(model=model)\n        self.worker = Worker(model=model)\n        self.solver = Solver(model=model)\n\n    def run(self, task, examples, verbose=False):\n        \"\"\"Run the PWS on a given task based on provided examples\n        Parameters:\n        ------------\n        task: str\n            Task for which the PWS is to be run\n        examples: list(str)\n            Examples related to the task for the fewshot prompt\n        verbose: bool, default=False\n            If True, responses from intermediate nodes are also returned\n\n        Returns:\n        ------------\n        pws_response: dict(str:obj)\n            PWS response contains the output and time elapsed\n            If verbose responses from intermediate nodes are also returned\n        \"\"\"\n\n        st = time.time()\n        # Plan\n        planner_response = self.planner.run(task, examples)\n        plans = planner_response[\"plans\"]\n        tool_calls = planner_response[\"tool_calls\"]\n\n        # Work\n        evidences = self.worker.run(tool_calls)\n\n        # Solve\n        output = self.solver.run(task, plans, evidences)\n\n        wall_time = time.time() - st\n\n        pws_response = {\"output\": output,\n                        \"wall_time\": wall_time}\n\n        if verbose:\n            pws_response[\"planner_response\"] = planner_response\n            pws_response[\"worker_response\"] = evidences\n\n        return pws_response\n\n\nclass EPS:\n    \"\"\" Evolutionary Prompt Selection\"\"\"\n    def __init__(self, index, embedding_model,\n                 similar_pool_size=5, instructive_pool_size=5):\n        self.index = index\n        index_stats = self.index.describe_index_stats()\n        self.index_size = index_stats['total_vector_count']\n        self.embedding_model = embedding_model\n        self.similar_pool_size = similar_pool_size\n        self.instructive_pool_size = instructive_pool_size\n        self.most_instructive = []\n        self.set_most_instructive()\n\n    def set_most_instructive(self):\n        \"\"\"Retrieve the most instructive examples from the index\n        Pinecone does not support aggregations over metadata so we fetch all\n        instructions and manually select the most instructive ones\n        \"\"\"\n        batch_size = 1000\n        score = lambda entry: entry['metadata']['score']\n        for i in range(0, self.index_size, batch_size):\n            # Find end of batch\n            i_end = min(i+batch_size, self.index_size)\n            # Create IDs batch\n            ids = [str(idx) for idx in range(i, i_end)]\n            batch = list(self.index.fetch(ids)['vectors'].values())\n            # Sort and keep the most instructive\n            batch_sorted = sorted(batch + self.most_instructive,\n                                  key=score, reverse=True)\n            self.most_instructive = batch_sorted[:self.instructive_pool_size]\n\n    def select_examples(self, task, num_examples=3):\n        \"\"\"Select instructive examples based on a given task\n        This method samples instructions from a curated pool of examples\n        The pool is curated by combinining (similar_pool_size) number of\n        semantically similar examples and (instructive_pool_size) number of\n        examples with high instruction score\n        The examples are then sampled based on their combined example score:\n        ((semantic similarity + 1.0) * (log(instruction_score) + 1.0)\n        Parameters:\n        ------------\n        task: str\n            Task for which the instructive examples are to be selected\n        nun_examples: int, default=3\n            Number of instructive examples to return\n\n        Returns:\n        ------------\n        examples: list(str)\n            List of instructive examples relevant to the task\n        \"\"\"\n        task_embedding = self.embedding_model.encode(task,\n            show_progress_bar=False).tolist()\n        most_similar = self.index.query(task_embedding,\n                                        top_k=self.similar_pool_size,\n                                        include_metadata=True)['matches']\n        instructive_ids = [entry['metadata']['id']\n                           for entry in self.most_instructive]\n        most_instructive = self.index.query(task_embedding,\n            top_k=self.instructive_pool_size,\n            filter={'id':{\"$in\": instructive_ids}},\n            include_metadata=True)['matches']\n        pool = most_similar + most_instructive\n        weights = [(entry['score'] + 1.0) * (math.log(\n            entry['metadata']['score']) + 1.0) for entry in pool]\n        probabilities = list(map(lambda weight: weight/sum(weights), weights))\n        sample_ids = choice(range(len(pool)), num_examples,\n                            replace=False, p=probabilities)\n        examples = [pool[i] for i in sample_ids]\n        return examples\n\n    def increment_score(self, entry_id):\n        \"\"\"Increment the instruction score of an example\n        Parameters:\n        ------------\n        entry_id: str\n            Score of the example with the given entry_id is incremented\n        \"\"\"\n        score = lambda entry: entry['metadata']['score']\n        entry = None\n        # Check if the entry is in the most instructive pool\n        for candidate in self.most_instructive:\n            if candidate['id'] == entry_id:\n                entry = candidate\n        # If the entry is not in the most instructive pool\n        if not entry:\n            # Fetch it from the index \n            entry = self.index.fetch([entry_id])['vectors'][entry_id]\n            # Add the entry to the most instructive pool\n            self.most_instructive.append(entry)\n        # Update entry score   \n        entry['metadata']['score'] += 1\n        self.index.update(id=entry['id'], set_metadata={\"score\": score(entry)})\n        # Sort the most instructive pool and keep the most instructive\n        self.most_instructive = sorted(self.most_instructive,\n                                       key=score, reverse=True)\n        self.most_instructive = self.most_instructive[\n            :self.instructive_pool_size]\n\n    def upsert_entry(self, metadata):\n        \"\"\"Upsert a new entry into the index\n        Parameters:\n        ------------\n        metadata: dict\n            a dictionary containing entry metadata\n            {question, plan, tools, dataset_name}\n        \"\"\"\n        entry_id = self.index_size\n        embedding = self.embedding_model.encode(metadata['question'],\n                                                show_progress_bar=False).tolist()\n        metadata['id'] = entry_id\n        metadata['score'] = 1\n        self.index.upsert(zip([str(entry_id)], [embedding], [metadata]))\n        self.index_size += 1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test the system","metadata":{}},{"cell_type":"markdown","source":"#### Define variables","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nPINECONE_API_KEY = user_secrets.get_secret('PINECONE_API_KEY')\nPINECONE_ENV = user_secrets.get_secret('PINECONE_ENVIRONMENT')\nINDEX_NAME = 'plans'\n\nEMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n\nMODEL_PATH = \"stabilityai/StableBeluga-7B\"\nSYSTEM_TAG = \"### System:\\n\"\nUSER_TAG = \"### User:\\n\"\nAI_TAG = \"### Assistant:\\n\"\n\nLOAD_IN_8BIT = True\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nTEMPERATURE = 0.01\nTOP_K = 50\nTOP_P = 0.9\nREPETITION_PENALTY= 1.0\nMAX_NEW_TOKENS = 256\nDEVICE_COUNT = 'auto'\n\nDATASET_NAME = \"trivia_qa\"\n\nSIMILAR_POOL_SIZE = 5\nINSTRUCTIVE_POOL_SIZE = 5\nNUM_EXAMPLES = 3\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Initialize models and prepare the data","metadata":{}},{"cell_type":"code","source":"generation_config = GenerationConfig(\n    do_sample=True,\n    temperature=TEMPERATURE,\n    top_k=TOP_K,\n    top_p=TOP_P,\n    repetition_penalty=REPETITION_PENALTY,\n    max_new_tokens=MAX_NEW_TOKENS\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(MODEL_PATH, generation_config=generation_config,\n                      load_in_8bit=LOAD_IN_8BIT, access_token=HF_TOKEN,\n                      system_tag=SYSTEM_TAG, user_tag=USER_TAG, ai_tag=AI_TAG)\nagent = PWS(model)\nextractor = Extractor(model)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pinecone.init(\n    api_key=PINECONE_API_KEY,\n    environment=PINECONE_ENV\n)\nindex = pinecone.GRPCIndex(INDEX_NAME)\n\nembedding_model = SentenceTransformer(EMBEDDING_MODEL)\n\nprompter = EPS(index, embedding_model, SIMILAR_POOL_SIZE, INSTRUCTIVE_POOL_SIZE)\n\ndataset = load_dataset(DATASET_NAME, 'rc.nocontext')\n\nsanitize = lambda text: text.strip().lower().translate(str.maketrans('', '', string.punctuation))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Hyperparameter Optimization","metadata":{}},{"cell_type":"code","source":"hp_opt_size = 20\nhp_opt_data = dataset['train'][:hp_opt_size]\ntemp_values = [0.01, 0.25, 0.5, 0.75, 1.0]\nrep_values = [1.0, 1.1, 1.2, 1.3]\nresults = []\nfor temp in temp_values:\n    for rep in rep_values:\n        generation_config = GenerationConfig(\n            do_sample=True,\n            temperature=temp,\n            top_k=TOP_K,\n            top_p=TOP_P,\n            repetition_penalty=rep,\n            max_new_tokens=MAX_NEW_TOKENS\n        )\n        model.generation_config = generation_config\n        em = []\n        for question, answer in tqdm(zip(hp_opt_data['question'], hp_opt_data['answer']),\n                                     total=hp_opt_size):\n            list_of_candidates = [sanitize(alias) for alias in answer[\"aliases\"]]\n\n            selection = prompter.select_examples(question, NUM_EXAMPLES)\n            examples = [entry['metadata'] for entry in selection]\n            response = agent.run(question, examples)\n            answer = sanitize(response['output'])\n\n            if answer not in list_of_candidates:\n                extracted = sanitize(extractor(response['output'], question))\n                if extracted not in list_of_candidates:\n                    em.append(False)\n                    continue \n            em.append(True)\n\n        print(f\"Temperature: {temp}\\nRepetition Penalty: {rep}\\nScore: {sum(em)}\\n\")\n        results.append({'temp':temp, 'rep':rep, 'score':sum(em)})\n    \nwith open(\"results.json\", \"w\") as f:\n    json.dump(results, f)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('results.json', 'r') as f:\n    results = json.load(f)\n    \nresults_df = pd.DataFrame.from_dict(results)\nresults_df = results_df.rename(columns={'temp': 'Temperature',\n                                        'rep': 'Repetition Penalty',\n                                        'score': 'EM Score'})\nresults_df = results_df.pivot(index='Temperature', columns='Repetition Penalty')\nresults_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Hyperparameter Optimization Round 2","metadata":{}},{"cell_type":"code","source":"hp_opt_size = 50\nhp_opt_data = dataset['train'][:hp_opt_size]\nconfigs = [(0.01, 1.0), (0.5, 1.1)]\nresults_v2 = []\nfor config in configs:\n    temp = config[0]\n    rep = config[1]\n    generation_config = GenerationConfig(\n        do_sample=True,\n        temperature=temp,\n        top_k=TOP_K,\n        top_p=TOP_P,\n        repetition_penalty=rep,\n        max_new_tokens=MAX_NEW_TOKENS\n    )\n    model.generation_config = generation_config\n    em = []\n    for question, answer in tqdm(zip(hp_opt_data['question'], hp_opt_data['answer']),\n                                 total=hp_opt_size):\n        list_of_candidates = [sanitize(alias) for alias in answer[\"aliases\"]]\n\n        selection = prompter.select_examples(question, NUM_EXAMPLES)\n        examples = [entry['metadata'] for entry in selection]\n        response = agent.run(question, examples)\n        answer = sanitize(response['output'])\n\n        if answer not in list_of_candidates:\n            extracted = sanitize(extractor(response['output'], question))\n            if extracted not in list_of_candidates:\n                em.append(False)\n                continue \n        em.append(True)\n\n    print(f\"Temperature: {temp}\\nRepetition Penalty: {rep}\\nScore: {sum(em)}\\n\")\n    results_v2.append({'temp':temp, 'rep':rep, 'score':sum(em)})\n\nwith open(\"results_v2.json\", \"w\") as f:\n    json.dump(results_v2, f)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run experiments","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:48:29.115409Z","iopub.status.idle":"2023-07-24T20:48:29.116144Z","shell.execute_reply.started":"2023-07-24T20:48:29.115892Z","shell.execute_reply":"2023-07-24T20:48:29.115917Z"}}},{"cell_type":"code","source":"def process_chunk(model, data, prompter, lock, thread_id, batch_offset):\n    print(f\"Thread {thread_id} started.\\n\")\n    # Initialize the agent and the extractor\n    agent = PWS(model)\n    extractor = Extractor(model)\n    # Initilize loop variables\n    batch_id = thread_id\n    results = []\n    for i, (question, answer) in enumerate(data):\n        # Process and save results for each batch\n        if i and not i % 100:\n            acc = sum([result['em'] for result in results]) / 100\n            print(f\"Processed batch number {batch_id} with {acc} accuracy.\")\n            with open(f\"results/results_batch_{batch_id}.json\", \"w\") as f:\n                json.dump(results, f)\n            batch_id += batch_offset\n            results = []\n        # Select examples using the prompter\n        lock.acquire()\n        selection = prompter.select_examples(question, NUM_EXAMPLES)\n        lock.release()\n        # Run the agent\n        examples = [entry['metadata'] for entry in selection]\n        response = agent.run(question, examples, verbose=True)\n        # Check the correctness of the answer\n        list_of_candidates = [sanitize(alias) for alias in answer[\"aliases\"]]\n        if sanitize(response['output']) in list_of_candidates:\n            em = True\n        else:\n            # Try extracting the answer from the output\n            extracted_output = extractor(response['output'], question)\n            if sanitize(extracted_output) in list_of_candidates:\n                em = True\n            else:\n                em = False\n        \n        instructions = [{'id': entry['id'],\n                         'similarity': entry['score']\n                        }\n                        for entry in selection]\n        results.append({'em': em, 'instructions': instructions})\n        # In case of an exact match, add the new plans to the index\n        # and increment the scores of the selected instructions\n        if em:\n            # Aggregate the tools used for this instance\n            tools = set()\n            for calls in response['planner_response']['tool_calls'].values():\n                tool = calls.split('[', 1)[0]\n                tools.add(tool)\n            tools = list(tools)\n            # Metadata for the new plans\n            new_entry_metadata = {'question': question,\n                                  'plan': response['planner_response']['text'],\n                                  'tools': tools,\n                                  'dataset_name': DATASET_NAME,  \n            }\n            lock.acquire()\n            # Add new plans to the index \n            prompter.upsert_entry(new_entry_metadata)\n            # Increment scores of the selected instructions\n            for entry in selection:\n                prompter.increment_score(entry['id'])\n            lock.release()\n    # Process and save results for the last batch    \n    acc = sum([result['em'] for result in results]) / len(results)\n    print(f\"Processed batch number {batch_id} with {acc} accuracy.\")\n    with open(f\"results/results_batch_{batch_id}.json\", \"w\") as f:\n        json.dump(results, f)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEVICE_COUNT == 'auto':\n    device_count = torch.cuda.device_count()\nelse:\n    device_count = DEVICE_COUNT\n    \ndataset_size = dataset['train'].num_rows\nchunk_size = int(math.ceil(dataset_size / device_count))\nlock = Lock()\nthreads = []\nfor device in range(device_count):\n    chunk = dataset['train'][(device * chunk_size):((device + 1) * chunk_size)]\n    data = zip(chunk['question'], chunk['answer'])\n    model = LanguageModel(MODEL_PATH, generation_config=generation_config,\n                          device_map=device, load_in_8bit=LOAD_IN_8BIT, access_token=HF_TOKEN,\n                          system_tag=SYSTEM_TAG, user_tag=USER_TAG, ai_tag=AI_TAG)\n    args = args=(model, data, prompter, lock, device, device_count)\n    threads.append(Thread(target=process_chunk, args=args))\n\nos.mkdir('results')\nfor thread in threads:\n    thread.start()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}