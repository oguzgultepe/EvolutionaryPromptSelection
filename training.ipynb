{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Evolutionary Prompt Selection\n## Trivia-QA Training","metadata":{}},{"cell_type":"markdown","source":"### Install Dependencies","metadata":{}},{"cell_type":"code","source":"!printf 'accelerate\\nbitsandbytes\\ndatasets\\npinecone-client[grpc]\\nsentencepiece\\nsentence-transformers\\ntorch\\ntransformers\\nwikipedia ' > requirements.txt  \n!pip install -r requirements.txt\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Statements","metadata":{"execution":{"iopub.status.busy":"2023-08-01T14:30:56.361309Z","iopub.execute_input":"2023-08-01T14:30:56.361651Z","iopub.status.idle":"2023-08-01T14:30:56.369346Z","shell.execute_reply.started":"2023-08-01T14:30:56.361606Z","shell.execute_reply":"2023-08-01T14:30:56.368460Z"}}},{"cell_type":"code","source":"import json\nimport math\nimport os\nimport string\nimport time\n\nimport pinecone\nimport torch\n\nfrom accelerate import Accelerator, notebook_launcher\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import GenerationConfig\nfrom tqdm.auto import tqdm\n\nfrom utils import LanguageModel, EPS, PWS\nfrom nodes import Extractor\n\nfrom kaggle_secrets import UserSecretsClient\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Global Variables","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nPINECONE_API_KEY = user_secrets.get_secret('PINECONE_API_KEY')\nPINECONE_ENV = user_secrets.get_secret('PINECONE_ENVIRONMENT')\nINDEX_NAME = 'plans'\n\nEMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n\nMODEL_PATH = \"stabilityai/StableBeluga-7B\"\nHF_TOKEN = None\nLOAD_IN_8BIT = True\nDEVICE_COUNT = 'auto'\n\nSYSTEM_TAG = \"### System:\\n\"\nUSER_TAG = \"### User:\\n\"\nAI_TAG = \"### Assistant:\\n\"\n\nTEMPERATURE = 0.01\nTOP_K = 50\nTOP_P = 0.9\nREPETITION_PENALTY= 1.0\nMAX_NEW_TOKENS = 256\n\nDATASET_NAME = \"trivia_qa\"\n\nNUM_EXAMPLES = 3\nBATCH_SIZE = 100\n\nRESULTS_DIR = 'results/'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define helper functions","metadata":{}},{"cell_type":"code","source":"sanitize = lambda text: text.strip().lower().translate(\n    str.maketrans('', '', string.punctuation)\n)\nget_path = lambda b_id: f\"{RESULTS_DIR}results_batch_{b_id}.json\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define the training function","metadata":{}},{"cell_type":"code","source":"def main():\n    # Initialize accelerator\n    accelerator = Accelerator(mixed_precision=\"fp16\")\n    accelerator.print('Accelerator initialized...')\n    # Initialize the database connection\n    accelerator.print('Initializing database connections...')\n    pinecone.init(\n        api_key=PINECONE_API_KEY,\n        environment=PINECONE_ENV\n    )\n    index = pinecone.GRPCIndex(INDEX_NAME)\n    # Load the dataset, use main_process_first to download only once\n    accelerator.print('Loading the dataset...')\n    with accelerator.main_process_first():\n        dataset = load_dataset(DATASET_NAME, 'rc.nocontext')\n    # Initialize the models\n    accelerator.print('Initializing the LLMs...')\n    generation_config = GenerationConfig(\n        do_sample=True,\n        temperature=TEMPERATURE,\n        top_k=TOP_K,\n        top_p=TOP_P,\n        repetition_penalty=REPETITION_PENALTY,\n        max_new_tokens=MAX_NEW_TOKENS\n    )\n    # Load the LLM, use main_process_first to download only once\n    with accelerator.main_process_first():\n        model = LanguageModel(\n            MODEL_PATH, generation_config=generation_config,\n            device_map=accelerator.device, load_in_8bit=LOAD_IN_8BIT,\n            system_tag=SYSTEM_TAG, user_tag=USER_TAG, ai_tag=AI_TAG\n        )\n    # Load the embedding model, use main_process_first to download only once\n    accelerator.print('Initializing the embedding models...')\n    with accelerator.main_process_first():\n        embedding_model = SentenceTransformer(EMBEDDING_MODEL,\n                                              device=accelerator.device)\n    accelerator.print('Preparing the LLMs...')\n    model = accelerator.prepare(model)\n    # Initialize the evolutionary prompter\n    accelerator.print('Initializing prompters...')\n    prompter = EPS(index, embedding_model)\n    # Initialize the agent and the extractor\n    accelerator.print('Initializing agents...')\n    agent = PWS(model)\n    extractor = Extractor(model)\n    # Breakdown data to chunks\n    accelerator.print('Chunking data...')\n    process_id = accelerator.process_index\n    num_processes = accelerator.num_processes\n    dataset_size = dataset['train'].num_rows\n    chunk_size = int(math.ceil(dataset_size / num_processes))\n    data_start = process_id * chunk_size\n    data_end = data_start + chunk_size\n    chunk = dataset['train'][data_start: data_end]\n    data = zip(chunk['question'], chunk['answer'])\n    # Initilize loop variables \n    accelerator.print('Starting training loop...')\n    batch_id = process_id\n    batch_offset = num_processes\n    results = []\n    # Initialize the results directory only once\n    if accelerator.is_main_process:\n        os.makedirs(RESULTS_DIR, exist_ok=True)\n    for i, (question, answer) in tqdm(enumerate(data), total=chunk_size,\n                                      disable=not accelerator.is_main_process):\n        # Process and save results for each batch\n        if i and not i % BATCH_SIZE:\n            acc = sum([result['em'] for result in results]) / BATCH_SIZE\n            print(f\"Processed batch number {batch_id} with {acc} accuracy.\")\n            with open(get_path(batch_id), \"w\") as f:\n                json.dump(results, f)\n            batch_id += batch_offset\n            results = []\n\n        # Select examples using the prompter\n        selection = prompter.select_examples(question, NUM_EXAMPLES)\n        # Run the agent\n        examples = [entry['metadata'] for entry in selection]\n        response = agent.run(question, examples, verbose=True)\n        # Check the correctness of the answer\n        list_of_candidates = [sanitize(alias) for alias in answer[\"aliases\"]]\n        if sanitize(response['output']) in list_of_candidates:\n            em = True\n        else:\n            # Try extracting the answer from the output\n            extracted_output = extractor(response['output'], question)\n            if sanitize(extracted_output) in list_of_candidates:\n                em = True\n            else:\n                em = False\n        instructions = [{'id': entry['id'],\n                         'similarity': entry['score']\n                        }\n                        for entry in selection]\n        results.append({'em': em, 'instructions': instructions})\n        # In case of an exact match, add the new plans to the index\n        # and increment the scores of the selected instructions\n        if em:\n            # Aggregate the tools used for this instance\n            tools = set()\n            for calls in response['planner_response']['tool_calls'].values():\n                tool = calls.split('[', 1)[0]\n                tools.add(tool)\n            tools = list(tools)\n            # Metadata for the new plans\n            new_entry_metadata = {'question': question,\n                                  'plan': response['planner_response']['text'],\n                                  'tools': tools,\n                                  'dataset_name': DATASET_NAME,\n            }\n            # Add new plans to the index \n            prompter.upsert_entry(new_entry_metadata)\n            # Increment scores of the selected instructions\n            prompter.increment_score([entry['id'] for entry in selection])\n    # Process and save results for the last batch\n    acc = sum([result['em'] for result in results]) / len(results)\n    print(f\"Processed batch number {batch_id} with {acc} accuracy.\")\n    with open(get_path(batch_id), \"w\") as f:\n        json.dump(results, f)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Launch Trainer","metadata":{}},{"cell_type":"code","source":"notebook_launcher(main, num_processes=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}